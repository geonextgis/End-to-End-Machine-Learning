{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d49e6e-94f9-415c-b51d-271b6e96f745",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Batch Gradient Descent**\n",
    "Batch Gradient Descent is a popular optimization algorithm used in machine learning and deep learning for training models, particularly for supervised learning tasks like linear regression and neural network training. It's a type of gradient descent algorithm that updates the model's parameters based on the average gradient of the loss function with respect to the **entire training dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c441c-e133-4b8c-ae52-f9ce7db208c7",
   "metadata": {},
   "source": [
    "<center><img src=\"https://editor.analyticsvidhya.com/uploads/58182variations_comparison.png\" style=\"width: 60%\"></ceneter>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd8c0e-b789-46d9-8687-ac5baf4911db",
   "metadata": {},
   "source": [
    "Here's how Batch Gradient Descent works:\n",
    "\n",
    "1. **Initialization**: Initialize the model parameters randomly or with some predefined values.\n",
    "\n",
    "2. **Batch Selection**: Divide the training dataset into smaller subsets called batches. Each batch contains a fixed number of training examples. The batch size is a hyperparameter that you can choose, and it determines how many examples are used in each parameter update.\n",
    "\n",
    "3. **Compute Gradient**: For each batch, compute the gradient of the loss function with respect to the model parameters. This gradient represents the direction and magnitude of the steepest ascent of the loss function for that batch.\n",
    "\n",
    "4. **Update Parameters**: Update the model parameters by moving in the opposite direction of the gradient. The update rule typically follows this formula:\n",
    "\n",
    "   ```\n",
    "   θ_new = θ_old - learning_rate * (∇(Loss) / ∇(θ))\n",
    "   ```\n",
    "\n",
    "   where:\n",
    "   - θ_new is the updated parameter values.\n",
    "   - θ_old is the current parameter values.\n",
    "   - learning_rate is a hyperparameter that controls the step size or learning rate of the optimization.\n",
    "   - ∇(Loss) represents the gradient of the loss function.\n",
    "   - ∇(θ) represents the gradient with respect to the model parameters.\n",
    "\n",
    "5. **Repeat**: Repeat steps 3 and 4 for all the batches in the training dataset. This constitutes one epoch.\n",
    "\n",
    "6. **Convergence Check**: Monitor the convergence of the algorithm by checking if the loss function has sufficiently decreased or other convergence criteria are met. If not, repeat steps 3 to 5 for more epochs.\n",
    "\n",
    "7. **Termination**: Stop the training process when a stopping criterion is met. This could be a maximum number of epochs, a certain level of loss convergence, or other criteria.\n",
    "\n",
    "Batch Gradient Descent has some advantages, such as stable and consistent updates, but it can be computationally expensive, especially when working with large datasets. It also requires the entire dataset to fit in memory, which may not be feasible for very large datasets. In such cases, variants of gradient descent, like Stochastic Gradient Descent (SGD) and Mini-Batch Gradient Descent, are often used to overcome these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2349c-8907-4566-80c0-2de3f18419b1",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a65374e-bbd1-453c-b73b-c5a2c3b63941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d228d-6b14-4ccc-bf03-fa00d4523cfa",
   "metadata": {},
   "source": [
    "## **Load a Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adafba9f-3fb9-4e72-8162-bab1cc797075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee58b2c-b6b6-4971-85bf-47360ebe0651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the Diabetes data\n",
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bd7e2d-3daa-44d1-b8d1-00efbc9320b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40be2d-48a6-4d35-a01a-7f449f209d17",
   "metadata": {},
   "source": [
    "## **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8696e3d-456e-4d62-8ed8-d9d5cef1c0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990d68de-b332-45de-8cd7-b745bf676b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((309, 10), (133, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6be0f8-1d54-42ca-94fd-ac4d2acd984d",
   "metadata": {},
   "source": [
    "## **Apply Linear Regression with Ordinary Least Squares (OLS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8d1c22-6b4c-4bd7-bced-6d589cd8c69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3cff662-bd5e-48b3-8294-394c2cfe2827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a linear regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73391ef1-70e4-4902-bf66-e27ab0e71af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [ -52.46478548 -193.50733393  579.49108514  272.453666   -504.64830389\n",
      "  241.62372969  -69.76596029   86.61313961  721.92083806   26.78067442] \n",
      "\n",
      "Intercept: 153.71901624380382\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\\n\", lr.coef_, \"\\n\")\n",
    "print(\"Intercept:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef37683-29ec-43e9-854d-90fdbc9349e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55bcba98-7f40-4bf1-bee6-1840d119ead4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the R2 Score\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d977b3e-3898-4d56-a405-ea822104d283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.39289927216962917\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32391efb-36a7-4cb9-80ff-3ad1e650eb74",
   "metadata": {},
   "source": [
    "## **Apply Multiple Linear Regression with Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "342045c4-81f2-4034-ac8e-89e6a19dc773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a class to apply gradient descent\n",
    "class GDRegressor:\n",
    "    # Constructor\n",
    "    def __init__(self, lr=0.01, epochs=100):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # Update all the coefficients and intercept value\n",
    "            y_hat = np.dot(X_train, self.coef_) + self.intercept_\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat), X_train) / X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "              \n",
    "    def predict(self, X_test):\n",
    "        return np.dot(X_test, self.coef_) + self.intercept_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ddcbf5-8c63-4b56-a654-88faf7e0a9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a GDRegressor object\n",
    "gdr = GDRegressor(lr=0.3, epochs=1000)\n",
    "\n",
    "# Fit the data\n",
    "gdr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d2f15d3-7d58-46dd-be25-d726590696bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [ -10.65736923 -127.14527905  460.18637967  255.19479606  -17.543418\n",
      "  -87.86651394 -216.91367665  134.31778094  394.43150793  113.12928298] \n",
      "\n",
      "Intercept: 153.3138537738541\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\\n\", gdr.coef_, \"\\n\")\n",
    "print(\"Intercept:\", gdr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bca5551-d09f-4401-910c-ac3964cdb3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "y_pred = gdr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5f7620-092a-43f0-9448-29db9f9f44c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.409476622848186\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R2 Score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
