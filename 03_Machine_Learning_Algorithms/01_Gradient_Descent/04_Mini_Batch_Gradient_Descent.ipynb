{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1504651c-179c-4401-b8d2-9bbfe0064034",
   "metadata": {},
   "source": [
    "# **Mini-Batch Gradient Descent**\n",
    "Mini-Batch Gradient Descent is a compromise between Stochastic Gradient Descent (SGD) and Batch Gradient Descent. In Mini-Batch Gradient Descent, the dataset is divided into small batches, and the model parameters are updated based on the average gradient of the loss function computed over each batch. This approach combines some of the advantages of both SGD and Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b292f4b-981a-4fed-bbbe-4329c94cffa5",
   "metadata": {},
   "source": [
    "<center><img src=\"https://editor.analyticsvidhya.com/uploads/58182variations_comparison.png\" style=\"width: 60%\"></ceneter>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea702bbb-2d18-4c43-b405-cadb571baf61",
   "metadata": {},
   "source": [
    "Here's how Mini-Batch Gradient Descent works:\n",
    "\n",
    "1. **Initialization**: Start with an initial set of model parameters.\n",
    "\n",
    "2. **Data Batching**: Divide the training dataset into small batches. The size of these batches is a hyperparameter known as the batch size.\n",
    "\n",
    "3. **Data Shuffling**: Optionally, shuffle the batches to introduce some randomness and ensure that the order of batches doesn't bias the training.\n",
    "\n",
    "4. **Iterative Updates**: For each training iteration (or epoch), process one mini-batch at a time. The model's parameters are updated based on the average gradient of the loss function computed over the data points in the mini-batch.\n",
    "\n",
    "5. **Gradient Computation**: Compute the gradient of the loss function with respect to the model parameters by backpropagating errors through the network (for neural networks) or using analytical derivatives (for simpler models).\n",
    "\n",
    "6. **Parameter Update**: Adjust the model parameters in the opposite direction of the average gradient. The learning rate controls the size of the step during each update.\n",
    "\n",
    "7. **Repeat**: Steps 4-6 are repeated for each mini-batch in the dataset until convergence criteria are met.\n",
    "\n",
    "Mini-Batch Gradient Descent has several advantages:\n",
    "\n",
    "- **Efficiency**: It takes advantage of vectorized operations, making it more computationally efficient than pure Stochastic Gradient Descent, especially when implemented on hardware that is optimized for matrix operations (e.g., GPUs).\n",
    "\n",
    "- **Regularization**: The mini-batch updates introduce a level of noise that can act as a form of regularization, potentially helping to prevent overfitting.\n",
    "\n",
    "- **Parallelization**: Mini-Batch Gradient Descent allows for parallelization, as multiple mini-batches can be processed simultaneously.\n",
    "\n",
    "- **Balanced Approach**: It strikes a balance between the high variance of SGD (processing one data point at a time) and the high computational requirements of Batch Gradient Descent (processing the entire dataset at once).\n",
    "\n",
    "The choice of the batch size is a crucial hyperparameter. A small batch size introduces more noise into the parameter updates but can provide faster convergence, while a larger batch size may result in more stable updates but slower convergence and potentially increased memory requirements.\n",
    "\n",
    "In practice, Mini-Batch Gradient Descent is widely used in training deep learning models due to its efficiency and balanced characteristics. The choice of batch size depends on factors such as the dataset size, available computational resources, and the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f5a06-1b70-4372-8b4a-7b15533beff2",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4d95b8-88e7-4d43-ad6e-663ff779bd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac885950-ae32-4e6d-b2f5-29bd7e0d0a34",
   "metadata": {},
   "source": [
    "## **Load a Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063ad20b-a0e6-4b65-b712-1bc8d4e98134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e398906-fece-433b-978c-ca0dacba393e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the Diabetes data\n",
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9621a6c-97e9-4fff-b6ef-12a4c5bc4ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40af443-af9e-46fa-a259-ca2d9b75892f",
   "metadata": {},
   "source": [
    "## **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9983fac0-9e3f-46cc-b860-7bbc897dcaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4f6826-131e-4181-937b-8679f366a5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((309, 10), (133, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cfa0f-b4f5-428d-84ab-3a167b890eac",
   "metadata": {},
   "source": [
    "## **Apply Linear Regression with Ordinary Least Squares (OLS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7507deda-b6f7-4248-a33f-b6aa75697406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5755855a-500c-412e-8da8-5f3afff6fa66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a 'LinearRegression' object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18541d72-efef-419a-9627-04b63b74e1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [ -52.46478548 -193.50733393  579.49108514  272.453666   -504.64830389\n",
      "  241.62372969  -69.76596029   86.61313961  721.92083806   26.78067442] \n",
      "\n",
      "Intercept: 153.71901624380382\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\\n\", lr.coef_, \"\\n\")\n",
    "print(\"Intercept:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c80a0a-4ecb-455a-9f9f-f047ed918498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6291f5-3e17-48f9-8521-3ac0db37bf9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the R2 Score\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c27af89-c3d3-4c31-91da-14183eeaeefd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.39289927216962917\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a0258-8847-4b37-9dbd-748b9dba2e5d",
   "metadata": {},
   "source": [
    "## **Apply Multiple Linear Regression with Mini-Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efee2849-f726-4f5b-a33f-860d2ec29509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f0ee03a-0e79-40e3-9320-2f4501024849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a class to apply gradient descent\n",
    "class MBGDRegressor:\n",
    "    def __init__(self, batch_size, lr=0.01, epochs=100):\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        # Initialize the coefficients\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                # Generate a list with random numbers \n",
    "                idx = random.sample(range(X_train.shape[0]), self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx], self.coef_) + self.intercept_\n",
    "                \n",
    "                # Update all the coefficients and intercept value\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "                \n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat), X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938f89b8-4cdd-481d-bbac-6a7dd686ac2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a GDRegressor object\n",
    "mbgdr = MBGDRegressor(batch_size=int(X_train.shape[0]/10), lr=0.01, epochs=50)\n",
    "\n",
    "# Fit the data\n",
    "mbgdr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3850c3c2-879c-4a5a-bdf8-6246b682c792",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [  23.72439516  -46.16118837  336.3399155   211.66412054   29.80588814\n",
      "  -18.56085148 -178.76484341  144.40792133  306.6760402   131.65463253] \n",
      "\n",
      "Intercept: 152.679170685235\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\\n\", mbgdr.coef_, \"\\n\")\n",
    "print(\"Intercept:\", mbgdr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5851f924-43f8-4165-b177-82bd5e14a254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "y_pred = mbgdr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c6d30d2-fbd5-4991-8846-aeff525799a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.3884193409712493\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04157cb-4a81-48ea-8a9a-d4f84bb5c9d0",
   "metadata": {},
   "source": [
    "## **Mini-Batch Gradient Descent with Scikit-Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89b44e61-281b-4da4-9213-ac5db4d7bf3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5920bb3-8bfb-47b5-85d4-1041300ffc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a SGDRegressor object\n",
    "reg = SGDRegressor(learning_rate=\"constant\", eta0=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd927302-e0e7-4cdd-94e6-51906022cf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the batch size and epochs\n",
    "batch_size = 35\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    idx = random.sample(range(X_train.shape[0]), batch_size)\n",
    "    reg.partial_fit(X_train[idx], y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "666f30fc-b28a-42be-87d6-96026ee8ac90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [ -13.88335269 -107.86708574  428.22522664  235.01421458    2.60971147\n",
      "  -71.34344956 -208.1173818   146.02324953  392.67322949  115.82512554] \n",
      "\n",
      "Intercept: [156.16418174]\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\\n\", reg.coef_, \"\\n\")\n",
    "print(\"Intercept:\", reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b90fd9b-c576-45ee-b1bf-a82567c2fcad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca50b6c0-ca6c-4064-8f1f-a566e143db51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.39806893761244444\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
